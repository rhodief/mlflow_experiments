# MLflow Experiments - ExplicaÃ§Ã£o Completa

## ğŸ¯ O que Ã© um Experimento no MLflow?

Um **Experimento** no MLflow Ã© um **container lÃ³gico** que agrupa mÃºltiplas execuÃ§Ãµes (runs) relacionadas a um mesmo projeto ou caso de uso de ML.

### Analogia:
```
Experimento = Projeto de ML
Run = Uma execuÃ§Ã£o especÃ­fica do pipeline
```

## ğŸ“Š Estrutura HierÃ¡rquica do MLflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MLflow Tracking Server                    â”‚
â”‚                    http://mlflow:5000                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                     â”‚
   Experiment 1                         Experiment 2
   "relatorio-II-iris-pipeline"        "outro-projeto"
        â”‚                                     â”‚
        â”œâ”€ Run 1 (DAG execution #1)          â”œâ”€ Run 1
        â”‚  â”œâ”€ Parameters                     â”‚  â””â”€ ...
        â”‚  â”œâ”€ Metrics                        â”‚
        â”‚  â”œâ”€ Artifacts                      â””â”€ Run 2
        â”‚  â””â”€ Tags                              â””â”€ ...
        â”‚
        â”œâ”€ Run 2 (DAG execution #2)
        â”‚  â”œâ”€ Parameters
        â”‚  â”œâ”€ Metrics
        â”‚  â”œâ”€ Artifacts
        â”‚  â””â”€ Tags
        â”‚
        â””â”€ Run 3 (DAG execution #3)
           â””â”€ ...
```

## âœ… Seu Setup Atual: CORRETO!

### VocÃª tem:

**1 Experimento**: `relatorio-II-iris-pipeline`
- Criado na task `ingestao_dados`
- Compartilhado por TODAS as tasks do DAG

**1 Run por execuÃ§Ã£o do DAG**:
- Criado na task `treinamento` com `mlflow.start_run()`
- O `run_id` Ã© passado para as tasks seguintes via XCom
- Todas as tasks logam no **mesmo run**

### Fluxo no seu DAG:

```python
# Task 1: ingestao_dados
mlflow.set_experiment("relatorio-II-iris-pipeline")  # â† Cria/usa experimento

# Task 3: treinamento
with mlflow.start_run(run_name="random_forest_training") as run:  # â† Cria RUN
    mlflow.log_params(...)      # Loga parÃ¢metros
    mlflow.sklearn.log_model()  # Loga modelo
    run_id = run.info.run_id    # Salva run_id

# Task 4: teste
with mlflow.start_run(run_id=run_id):  # â† USA O MESMO RUN
    mlflow.log_metric("accuracy", ...)
    mlflow.log_metric("precision", ...)
    # ... mais mÃ©tricas

# Task 5: empacotamento
with mlflow.start_run(run_id=run_id):  # â† USA O MESMO RUN
    mlflow.log_artifact(onnx_path)
```

## ğŸ“ˆ O que acontece em cada execuÃ§Ã£o do DAG?

### Primeira execuÃ§Ã£o:
```
Experimento: relatorio-II-iris-pipeline
â””â”€â”€ Run 1
    â”œâ”€â”€ Parameters: n_estimators=100, max_depth=10
    â”œâ”€â”€ Metrics: accuracy=0.9667, precision=0.9667, ...
    â”œâ”€â”€ Artifacts: modelo sklearn, ONNX, feature_importance
    â””â”€â”€ Tags: run_name="random_forest_training"
```

### Segunda execuÃ§Ã£o (se vocÃª executar de novo):
```
Experimento: relatorio-II-iris-pipeline
â”œâ”€â”€ Run 1 (execuÃ§Ã£o anterior)
â””â”€â”€ Run 2 (nova execuÃ§Ã£o)
    â”œâ”€â”€ Parameters: n_estimators=100, max_depth=10
    â”œâ”€â”€ Metrics: accuracy=0.9667, precision=0.9667, ...
    â””â”€â”€ Artifacts: modelo sklearn, ONNX, feature_importance
```

## ğŸ” Verificando no MLflow UI

### 1. Acesse: http://localhost:5000

### 2. Veja o Experimento:
- Clique em "Experiments" no menu lateral
- VocÃª verÃ¡: `relatorio-II-iris-pipeline`
- Ao lado do nome: **1 run** (ou mais se executou mÃºltiplas vezes)

### 3. Clique no Experimento:
VocÃª verÃ¡ uma **tabela com todos os runs**:

| Run Name | Created | Metrics | Parameters |
|----------|---------|---------|------------|
| random_forest_training | 2025-10-29 | acc: 0.9667 | n_est: 100 |

### 4. Clique em um Run:
VerÃ¡ todos os detalhes:
- **Parameters**: n_estimators, max_depth, random_state
- **Metrics**: accuracy, precision, recall, f1_score
- **Artifacts**: 
  - model/ (sklearn model)
  - onnx_model/ (ONNX model)
  - feature_importance.json
  - confusion_matrix.json
  - classification_report.txt

## â“ Quando ter mÃºltiplos Experimentos?

VocÃª criaria **experimentos separados** para:

### Exemplo 1: Diferentes Datasets
```python
# Experimento 1: Iris
mlflow.set_experiment("iris-classification")

# Experimento 2: Wine
mlflow.set_experiment("wine-classification")

# Experimento 3: Digits
mlflow.set_experiment("digits-classification")
```

### Exemplo 2: Diferentes Objetivos
```python
# Experimento 1: ExploraÃ§Ã£o inicial
mlflow.set_experiment("iris-exploration")

# Experimento 2: OtimizaÃ§Ã£o de hiperparÃ¢metros
mlflow.set_experiment("iris-hyperparameter-tuning")

# Experimento 3: ProduÃ§Ã£o
mlflow.set_experiment("iris-production")
```

### Exemplo 3: Diferentes Algoritmos (se fossem projetos separados)
```python
# Experimento 1: Random Forest
mlflow.set_experiment("iris-random-forest")

# Experimento 2: SVM
mlflow.set_experiment("iris-svm")

# Experimento 3: Neural Network
mlflow.set_experiment("iris-neural-network")
```

## âŒ VocÃª NÃƒO precisa de experimentos separados para cada task

**Errado** (nÃ£o faÃ§a isso):
```python
# Task 1
mlflow.set_experiment("relatorio-II-ingestao")

# Task 3
mlflow.set_experiment("relatorio-II-treinamento")  # âŒ Separa tudo

# Task 4
mlflow.set_experiment("relatorio-II-teste")  # âŒ Perde a conexÃ£o
```

**Certo** (o que vocÃª jÃ¡ tem):
```python
# Task 1: Define o experimento UMA vez
mlflow.set_experiment("relatorio-II-iris-pipeline")

# Task 3: Cria um RUN dentro do experimento
with mlflow.start_run() as run:
    # Treina e loga
    pass

# Task 4: USA O MESMO RUN
with mlflow.start_run(run_id=run_id):
    # Loga mÃ©tricas no mesmo run
    pass
```

## ğŸ“ Conceitos Importantes

### Experiment (Experimento)
- **O quÃª**: Container de runs relacionados
- **Quando criar**: Para cada projeto/caso de uso distinto
- **Quantos vocÃª tem**: 1 (`relatorio-II-iris-pipeline`) âœ…
- **Quantos vocÃª deveria ter**: 1 âœ…

### Run (ExecuÃ§Ã£o)
- **O quÃª**: Uma execuÃ§Ã£o especÃ­fica do seu cÃ³digo ML
- **Quando criar**: Cada vez que vocÃª treina um modelo
- **Quantos vocÃª tem**: 1 por execuÃ§Ã£o do DAG
- **Quantos vocÃª deveria ter**: Quantas vezes executar o DAG

### Parameters (ParÃ¢metros)
- **O quÃª**: ConfiguraÃ§Ãµes do modelo (ex: n_estimators=100)
- **Onde**: Dentro de cada run
- **Quando logar**: Durante o treinamento

### Metrics (MÃ©tricas)
- **O quÃª**: Resultados de avaliaÃ§Ã£o (ex: accuracy=0.9667)
- **Onde**: Dentro de cada run
- **Quando logar**: Durante o teste/avaliaÃ§Ã£o

### Artifacts (Artefatos)
- **O quÃª**: Arquivos gerados (modelos, grÃ¡ficos, etc.)
- **Onde**: Dentro de cada run
- **Quando logar**: Durante treinamento, empacotamento, etc.

## ğŸ“Š VisualizaÃ§Ã£o do seu Pipeline

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           Experimento: relatorio-II-iris-pipeline             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                               â•‘
â•‘  Run 1: random_forest_training                                â•‘
â•‘  â”œâ”€ Created by: task "treinamento"                           â•‘
â•‘  â”œâ”€ Run ID: abc123...                                        â•‘
â•‘  â”‚                                                            â•‘
â•‘  â”œâ”€ Parameters (logged by "treinamento")                     â•‘
â•‘  â”‚   â”œâ”€ n_estimators: 100                                    â•‘
â•‘  â”‚   â”œâ”€ max_depth: 10                                        â•‘
â•‘  â”‚   â””â”€ random_state: 42                                     â•‘
â•‘  â”‚                                                            â•‘
â•‘  â”œâ”€ Metrics (logged by "teste")                              â•‘
â•‘  â”‚   â”œâ”€ accuracy: 0.9667                                     â•‘
â•‘  â”‚   â”œâ”€ precision: 0.9667                                    â•‘
â•‘  â”‚   â”œâ”€ recall: 0.9667                                       â•‘
â•‘  â”‚   â””â”€ f1_score: 0.9667                                     â•‘
â•‘  â”‚                                                            â•‘
â•‘  â””â”€ Artifacts                                                 â•‘
â•‘      â”œâ”€ model/ (logged by "treinamento")                     â•‘
â•‘      â”‚   â””â”€ sklearn model                                    â•‘
â•‘      â”œâ”€ onnx_model/ (logged by "empacotamento")              â•‘
â•‘      â”‚   â””â”€ iris_random_forest.onnx                          â•‘
â•‘      â”œâ”€ feature_importance.json (logged by "treinamento")    â•‘
â•‘      â”œâ”€ confusion_matrix.json (logged by "teste")            â•‘
â•‘      â””â”€ classification_report.txt (logged by "teste")        â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## ğŸ” Comandos para Verificar

### Ver todos os experimentos
```bash
docker compose exec airflow-apiserver python -c "
import mlflow
mlflow.set_tracking_uri('http://mlflow:5000')
experiments = mlflow.search_experiments()
for exp in experiments:
    print(f'ID: {exp.experiment_id}, Nome: {exp.name}')
"
```

### Ver runs de um experimento
```bash
docker compose exec airflow-apiserver python -c "
import mlflow
mlflow.set_tracking_uri('http://mlflow:5000')
runs = mlflow.search_runs(experiment_names=['relatorio-II-iris-pipeline'])
print(runs[['run_id', 'metrics.accuracy', 'params.n_estimators']])
"
```

## âœ… Resumo: EstÃ¡ Tudo Certo!

| Item | Seu Setup | Status |
|------|-----------|--------|
| NÃºmero de experimentos | 1 | âœ… Correto |
| Nome do experimento | relatorio-II-iris-pipeline | âœ… Correto |
| Runs por execuÃ§Ã£o do DAG | 1 | âœ… Correto |
| Compartilhamento do run_id | Sim, via XCom | âœ… Correto |
| Logs no mesmo run | Sim | âœ… Correto |

## ğŸ¯ ConclusÃ£o

**SIM, ter 1 experimento estÃ¡ CORRETO!**

VocÃª verÃ¡:
- **1 Experimento**: `relatorio-II-iris-pipeline`
- **N Runs**: Um para cada vez que executar o DAG

Cada run conterÃ¡:
- ParÃ¢metros do modelo
- MÃ©tricas de avaliaÃ§Ã£o
- Artefatos (modelos, ONNX, reports)

Isso Ã© a **best practice** para um pipeline de ML integrado! ğŸ‰
